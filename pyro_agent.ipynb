{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "import pyro\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO, MCMC, NUTS\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.constraints as constraints\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\",category=FutureWarning)\n",
    "\n",
    "pyro.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rational agent\n",
    "\n",
    "The idea is to train n models where each model is trained using n trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract valid subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract_correct_csv\n",
    "valid_sub = extract_correct_csv.extract_only_valid_subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read real data of subject #2 (data equals to all subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "df = pd.read_csv('data/newLookAtMe/newLookAtMe02.csv')\n",
    "df_rational = df[['morphing level', 'shock']]\n",
    "df_rational['shock'] = df_rational['shock'].astype(int) #setting shock as int instead of boolean\n",
    "df_rational['morphing level'] = [int(d==6) for d in df_rational['morphing level']] # if morphing level==6 -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = df_rational.to_numpy()\n",
    "\n",
    "HABITUATION_TRIALS = 16\n",
    "ACQUISITION_TRIALS = 48\n",
    "data_all = data_np[16:]\n",
    "learning_data = data_np[HABITUATION_TRIALS:ACQUISITION_TRIALS] # remove only habituation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(data_all)\n",
    "N = data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_csplus_simulated = np.load('output/pyro/complete_rational/csplus.npy',allow_pickle=True)\n",
    "array_csminus_simulated = np.load('output/pyro/complete_rational/csminus.npy',allow_pickle=True)\n",
    "total_array_simulated = np.load('output/pyro/complete_rational/total.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shock expectancy simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df[['shock', 'rating', 'morphing level']]\n",
    "df_['shock'] = df_['shock'].astype(int)\n",
    "df_['morphing level'] = [int(d == 6) for d in df_['morphing level']]\n",
    "df_['rating'] = df_['rating'].replace([1, 2, 3, 4, 5], [0.2, 0.4, 0.6, 0.8, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove first 16 trials real data\n",
    "df_new=df_\n",
    "#consider only the learning phase\n",
    "df_learning = df_\n",
    "df_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_plus_real = df_new.loc[df_new['morphing level']==1].rating.values\n",
    "x_plus_real = np.array(df_new.loc[df_new['morphing level']==1].index)\n",
    "y_minus_real = df_new.loc[df_new['morphing level']==0].rating.values\n",
    "x_minus_real = np.array(df_new.loc[df_new['morphing level']==0].index)\n",
    "y_real=df_new.rating.values\n",
    "\n",
    "y_plus_train = df_learning.loc[df_learning['morphing level']==1].rating.values\n",
    "x_plus_train = np.array(df_learning.loc[df_learning['morphing level']==1].index)\n",
    "y_minus_train = df_learning.loc[df_learning['morphing level']==0].rating.values\n",
    "x_minus_train = np.array(df_learning.loc[df_learning['morphing level']==0].index)\n",
    "y_train=df_learning.rating.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13,6))\n",
    "plt.title('Analysis of the shock prediction comparing subject #2 and rational agent')\n",
    "plt.scatter(x_plus_real, y_plus_real, color='blue', label='cs+ real')\n",
    "plt.scatter(array_csplus_simulated[:,0], array_csplus_simulated[:,1], color='darkblue', label='cs+ simulated')\n",
    "plt.scatter(x_minus_real, y_minus_real, color='red', label='cs- real')\n",
    "plt.scatter(array_csminus_simulated[:,0], array_csminus_simulated[:,1], color='darkred', label='cs- simulated')\n",
    "plt.legend(loc='right')\n",
    "plt.axvline(x=16, linestyle='--', color='green')\n",
    "plt.axvline(x=48, linestyle='--', color='green')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('P(condition | visual stimulus)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# correlation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis of the correlation between rational agent and real subject with pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating between 16-160 trial rational agent\n",
    "rating_rational = total_array_simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correlation = pd.DataFrame(columns=['subject','pearson','r2score'])\n",
    "\n",
    "for sub in valid_sub:\n",
    "    subj_ = extract_correct_csv.read_correct_subject_csv(sub)\n",
    "    df_sub = pd.read_csv('data/newLookAtMe/newLookAtMe'+subj_+'.csv')\n",
    "    df_sub = df_sub[['shock', 'rating', 'morphing level']]\n",
    "    df_sub['shock'] = df_sub['shock'].astype(int)\n",
    "    df_sub['morphing level'] = [int(d == 6) for d in df_sub['morphing level']]\n",
    "    df_sub['rating'] = df_sub['rating'].replace([1, 2, 3, 4, 5], [0.2, 0.4, 0.6, 0.8, 1])\n",
    "    df_sub_learn = df_sub[16:]\n",
    "    rating_sub = np.array(df_sub_learn['rating'])\n",
    "\n",
    "    # remove nan\n",
    "    bad = ~np.logical_or(np.isnan(rating_sub), np.isnan(rating_rational))\n",
    "    rating_sub_ = np.compress(bad, rating_sub)\n",
    "    rating_rational_ = np.compress(bad, rating_rational)\n",
    "\n",
    "    #pearson corr coeff\n",
    "    pearson = round(np.corrcoef(rating_sub_,rating_rational_)[0][1],2)\n",
    "\n",
    "    #r2 score\n",
    "    r2 = round(r2_score(rating_sub_,rating_rational_),2)\n",
    "    df_tmp = pd.DataFrame({'subject':sub,'pearson':pearson,'r2score':r2},index=np.arange(1))\n",
    "    df_correlation = pd.concat([df_correlation,df_tmp])\n",
    "\n",
    "\n",
    "sias_df = pd.read_csv('data/sias_score.csv').drop(columns='social_anxiety')\n",
    "sias_df['subject'] = [float(x) for x in sias_df['subject']]\n",
    "\n",
    "lds_df = pd.read_csv('data/lds_subjects.csv')\n",
    "lds_df['subject'] = [float(x) for x in lds_df['subject']]\n",
    "\n",
    "df_correlation['subject'] = [float(x) for x in df_correlation['subject']]\n",
    "\n",
    "df_corr_ = pd.concat([sias_df.set_index('subject'), lds_df.set_index('subject'), df_correlation.set_index('subject')], axis=1).reset_index()\n",
    "\n",
    "# drop nan\n",
    "lds_df['subject'] = [float(x) for x in lds_df['subject']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged_df = pd.concat([sias_df.set_index('subject'), lds_df.set_index('subject'), df_correlation.set_index('subject')], axis=1)\n",
    "merged_df.reset_index()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df_correlation.join(lds_df, lsuffix='', rsuffix='_other',how='inner').drop(columns='subject_other')\n",
    "df_corr_ = df_corr.join(sias_df,lsuffix='', rsuffix='_other',how='inner').drop(columns='subject_other')\n",
    "df_corr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_notna = df_corr_[df_corr_['pearson'].notna()]\n",
    "df_corr_notna = df_corr_notna[df_corr_['lds'].notna()]\n",
    "df_corr_notna = df_corr_notna[df_corr_['sias_score'].notna()]\n",
    "\n",
    "#extract subject with high value lds and lower lds\n",
    "lds_ = df_corr_notna.sort_values('lds').reset_index().drop(columns='index')\n",
    "lower_lds = lds_[:7]\n",
    "higher_lds = lds_[-7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis subject with more fear gen vs subjects less fear gen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sias_ = df_corr_notna.sort_values('sias_score',ascending=True).reset_index().drop(columns='index')\n",
    "lower_sias = sias_[:5]\n",
    "sias_ = df_corr_notna.sort_values('sias_score',ascending=False).reset_index().drop(columns='index')\n",
    "higher_sias= sias_[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis subject more fear gen vs less fear gen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "0.46"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_low_p = lower_lds['pearson'].median()\n",
    "l_hig_p = higher_lds['pearson'].median()\n",
    "l_low_r = lower_lds['r2score'].median()\n",
    "l_hig_r = higher_lds['r2score'].median()\n",
    "\n",
    "print('Correlation between 7 more/less fear gen subjects with the rational agent\\n')\n",
    "print('Pearson\\nHigh fear gen: ',l_low_p, ' Low fear gen:',l_hig_p)\n",
    "print('\\nR2score\\nHigh fear gen: ',l_low_r, ' Low fear gen:',l_hig_r)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis subject more anxiety vs less anxiety"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s_low_p = lower_sias['pearson'].median()\n",
    "s_hig_p = higher_sias['pearson'].median()\n",
    "\n",
    "s_low_r = lower_sias['r2score'].median()\n",
    "s_hig_r = higher_sias['r2score'].median()\n",
    "\n",
    "print('Correlation between 5 more/less anxiety subjects with the rational agent\\n')\n",
    "print('Pearson\\nHigh anxiety: ',s_hig_p, ' Low anxiety:',s_low_p)\n",
    "print('\\nR2score\\nHigh anxiety: ',s_hig_r, ' Low anxiety:',s_low_r)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis subject with more anxiety vs subjects less anxiety"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sias_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_sias['pearson'].median()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "higher_sias['pearson'].median()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis sliding window K"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k_wind = [2, 5, 10, 25, 50, 100, 150]\n",
    "for k in list(k_wind):\n",
    "\n",
    "    array_csplus_simulated = np.load('output/pyro/sliding_wind/k'+str(k)+'_csplus.npy',allow_pickle=True)\n",
    "    array_csminus_simulated = np.load('output/pyro/sliding_wind/k'+str(k)+'_csminus.npy',allow_pickle=True)\n",
    "    total_array_simulated = np.load('output/pyro/sliding_wind/k'+str(k)+'_total.npy',allow_pickle=True)\n",
    "\n",
    "    rating_rational = total_array_simulated\n",
    "    rating_rational = rating_rational[16:]\n",
    "\n",
    "    df_correlation = pd.DataFrame(columns=['subject','pearson'])\n",
    "\n",
    "    for sub in valid_sub:\n",
    "        subj_ = extract_correct_csv.read_correct_subject_csv(sub)\n",
    "        df_sub = pd.read_csv('data/newLookAtMe/newLookAtMe'+subj_+'.csv')\n",
    "        df_sub = df_sub[['shock', 'rating', 'morphing level']]\n",
    "        df_sub['shock'] = df_sub['shock'].astype(int)\n",
    "        df_sub['morphing level'] = [int(d == 6) for d in df_sub['morphing level']]\n",
    "        df_sub['rating'] = df_sub['rating'].replace([1, 2, 3, 4, 5], [0.2, 0.4, 0.6, 0.8, 1])\n",
    "        df_sub_learn = df_sub[16:]\n",
    "        rating_sub = np.array(df_sub_learn['rating'])\n",
    "        bad = ~np.logical_or(np.isnan(rating_sub), np.isnan(rating_rational))\n",
    "        rating_sub_ = np.compress(bad, rating_sub)\n",
    "        rating_rational_ = np.compress(bad, rating_rational)\n",
    "        pearson = round(np.corrcoef(rating_sub_,rating_rational_)[0][1],2)\n",
    "        df_correlation = df_correlation.append({'subject':sub,'pearson':pearson},ignore_index=True)\n",
    "    sias_df = pd.read_csv('data/sias_score.csv').drop(columns='social_anxiety')\n",
    "    sias_df['subject'] = [float(x) for x in sias_df['subject']]\n",
    "\n",
    "    lds_df = pd.read_csv('data/lds_subjects.csv')\n",
    "\n",
    "    lds_df['subject'] = [float(x) for x in lds_df['subject']]\n",
    "    df_corr = df_correlation.join(lds_df, lsuffix='', rsuffix='_other',how='inner').drop(columns='subject_other')\n",
    "    df_corr_ = df_corr.join(sias_df,lsuffix='', rsuffix='_other',how='inner').drop(columns='subject_other')\n",
    "    df_corr_notna = df_corr_[df_corr_['pearson'].notna()]\n",
    "    df_corr_notna = df_corr_notna[df_corr_['lds'].notna()]\n",
    "    df_corr_notna = df_corr_notna[df_corr_['sias_score'].notna()]\n",
    "    lds_ = df_corr_notna.sort_values('lds').reset_index().drop(columns='index')\n",
    "    lower_lds = lds_[:7]\n",
    "    higher_lds = lds_[-7:]\n",
    "\n",
    "    print('\\n\\n--------------- K = '+str(k)+' ---------------')\n",
    "\n",
    "    print('7 subj less lds corr with rational agent:',lower_lds['pearson'].median())\n",
    "    print('7 subj more lds corr with rational agent:',higher_lds['pearson'].median())\n",
    "\n",
    "    sias_ = df_corr_notna.sort_values('sias_score',ascending=True).reset_index().drop(columns='index')\n",
    "    lower_sias = sias_[:7]\n",
    "    sias_ = df_corr_notna.sort_values('sias_score',ascending=False).reset_index().drop(columns='index')\n",
    "    higher_sias= sias_[:7]\n",
    "    print()\n",
    "    print('7 subj less sias corr with rational agent:',lower_sias['pearson'].median())\n",
    "    print('7 subj more sias corr with rational agent:',higher_sias['pearson'].median())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_corr_notna['sias_score'] = [round(x,2) for x in preprocessing.normalize([df_corr_notna['sias_score']])[0]]\n",
    "df_corr_notna['lds'] = [round(x,2) for x in preprocessing.normalize([df_corr_notna['lds']])[0]]\n",
    "df_corr_notna['pearson'] = [round(x,2) for x in preprocessing.normalize([df_corr_notna['pearson']])[0]]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_corr_notna['subject'] = [int(x) for x in df_corr_notna['subject']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lds_ = df_corr_notna.sort_values('lds').reset_index().drop(columns='index')\n",
    "lower_lds = lds_[:5]\n",
    "higher_lds = lds_[-5:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = tuple(df_corr_notna['subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {'pearson':(),'lds':(),'sias':()}\n",
    "for index,row in df_corr_notna.iterrows():\n",
    "    new_p = values['pearson'] + (row['pearson'],)\n",
    "    values.update({'pearson':new_p})\n",
    "    new_l = values['lds'] + (row['lds'],)\n",
    "    values.update({'lds':new_l})\n",
    "    new_s = values['sias'] + (row['sias_score'],)\n",
    "    values.update({'sias':new_s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_no_sias = {'pearson':(),'lds':()}\n",
    "for index,row in df_corr_notna.iterrows():\n",
    "    new_p = values_no_sias['pearson'] + (row['pearson'],)\n",
    "    values_no_sias.update({'pearson':new_p})\n",
    "    new_l = values_no_sias['lds'] + (row['lds'],)\n",
    "    values_no_sias.update({'lds':new_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = tuple(df_corr_notna['subject'])\n",
    "values_no_lds = {'pearson': (), 'sias': ()}\n",
    "for index, row in df_corr_notna.iterrows():\n",
    "    new_p = values_no_lds['pearson'] + (row['pearson'],)\n",
    "    values_no_lds.update({'pearson': new_p})\n",
    "    new_s = values_no_lds['sias'] + (row['sias_score'],)\n",
    "    values_no_lds.update({'sias': new_s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(subjects))  # the label locations\n",
    "width = 0.15  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8),constrained_layout=True)\n",
    "\n",
    "for attribute, measurement in values_no_lds.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    #ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Length (mm)')\n",
    "ax.set_title('correlation attributes by subjects')\n",
    "ax.set_xticks(x + width, subjects)\n",
    "ax.legend(loc='upper left', ncols=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(subjects))  # the label locations\n",
    "width = 0.15  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8),constrained_layout=True)\n",
    "\n",
    "for attribute, measurement in values_no_sias.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    #ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Length (mm)')\n",
    "ax.set_title('correlation attributes by subjects')\n",
    "ax.set_xticks(x + width, subjects)\n",
    "ax.legend(loc='upper left', ncols=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(subjects))  # the label locations\n",
    "width = 0.15  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8),constrained_layout=True)\n",
    "\n",
    "for attribute, measurement in values.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    #ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Length (mm)')\n",
    "ax.set_title('correlation attributes by subjects')\n",
    "ax.set_xticks(x + width, subjects)\n",
    "ax.legend(loc='upper left', ncols=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rational agent sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to have a rational agent with a limited memory over previous trials.\n",
    "k = param sliding window dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract_correct_csv\n",
    "valid_sub = extract_correct_csv.extract_only_valid_subject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "df = pd.read_csv('data/newLookAtMe/newLookAtMe02.csv')\n",
    "df_rational = df[['morphing level', 'shock']]\n",
    "df_rational['shock'] = df_rational['shock'].astype(int) #setting shock as int instead of boolean\n",
    "df_rational['morphing level'] = [int(d==6) for d in df_rational['morphing level']] # if morphing level==6 -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = df_rational.to_numpy()\n",
    "\n",
    "\n",
    "def counter_window(data, k=0):\n",
    "    N = data.shape[0]\n",
    "    counter = torch.zeros((N,4))\n",
    "    for i in range(len(data)):\n",
    "        dict_ = {'[0 0]':0, '[0 1]': 0, '[1 0]':0, '[1 1]':0}\n",
    "        if k == 0 or k > i:\n",
    "            tmp_data = data[:i+1]\n",
    "        else:\n",
    "            tmp_data = data[i-k:i+1]\n",
    "            #print('im here')\n",
    "        # count occurencies\n",
    "        for x in tmp_data:\n",
    "            dict_[str(x)] += 1\n",
    "        values = np.array(list(dict_.values()))\n",
    "        counter[i] = torch.tensor(values)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = counter_window(data_np, 3)\n",
    "\n",
    "counter = counter.reshape((len(data_np), 2, 2))\n",
    "counter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical/multinomial distribution\n",
    "\n",
    "# uniform prior\n",
    "prior_counts = torch.ones((2,2))\n",
    "\n",
    "\n",
    "#model\n",
    "def model(data):\n",
    "    prior = pyro.sample(\"prior\", dist.Dirichlet(prior_counts))\n",
    "    total_counts = int(data.sum())\n",
    "    pyro.sample(\"likelihood\", dist.Multinomial(total_counts, prior), obs=data)\n",
    "\n",
    "\n",
    "nuts_kernel = NUTS(model)\n",
    "num_samples, warmup_steps = (300, 200)\n",
    "\n",
    "mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps, disable_progbar=True)\n",
    "all_means = []\n",
    "\n",
    "# sampling\n",
    "for i in range(len(counter)):\n",
    "    mcmc.run(counter[i])\n",
    "    hmc_samples = {k: v.detach().cpu().numpy()\n",
    "                   for k, v in mcmc.get_samples().items()}\n",
    "    means = hmc_samples['prior'].mean(axis=0)\n",
    "    stds = hmc_samples['prior'].std(axis=0)\n",
    "    print('observation: ', data_np[i])\n",
    "    print('probabilities: ', means)\n",
    "    all_means.append(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Markov model\n",
    "\n",
    "![example](https://pyro.ai/examples/_static/img/model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "from os.path import exists\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.contrib.examples.polyphonic_data_loader as poly\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.distributions import TransformedDistribution\n",
    "from pyro.distributions.transforms import affine_autoregressive\n",
    "from pyro.infer import (\n",
    "    SVI,\n",
    "    JitTrace_ELBO,\n",
    "    Trace_ELBO,\n",
    "    TraceEnum_ELBO,\n",
    "    TraceTMC_ELBO,\n",
    "    config_enumerate,\n",
    ")\n",
    "from pyro.optim import ClippedAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emission function (Emit in the figure)\n",
    "class Emitter(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the bernoulli observation likelihood p(x_t|z_t)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, z_dim, emission_dim):\n",
    "        super().__init__()\n",
    "        # emission_dim is the number of hidden units in the neural network\n",
    "        # three linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, emission_dim)\n",
    "        self.lin_hidden_to_hidden = nn.Linear(emission_dim, emission_dim)\n",
    "        self.lin_hidden_to_input = nn.Linear(emission_dim, input_dim)\n",
    "        # two non linear used in the neural network\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z_t):\n",
    "        \"\"\"\n",
    "        Given the latent z at a particular time step t, we return the vector of probabilities 'ps' that parametrizes the bernoulli distribution p(x_t|z_t)\n",
    "        Taken together the elements of ps encode which notes we expect to observe at time t given the state of the system (as encoded in z_t).\n",
    "        \"\"\"\n",
    "        h1 = self.relu(self.lin_z_to_hidden(z_t))\n",
    "        h2 = self.relu(self.lin_hidden_to_hidden(h1))\n",
    "        ps = self.sigmoid(self.lin_hidden_to_input(h2))\n",
    "        return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gated transition (Trans in the figure above)\n",
    "class GatedTransition(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the gaussian latent transition probability p(z_t | z_{t-1})\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, transition_dim):\n",
    "        super().__init__()\n",
    "        # six linear transform\n",
    "        self.lin_gate_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_gate_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_proposed_mean_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_proposed_mean_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_sig = nn.Linear(z_dim, z_dim)\n",
    "        self.lin_z_to_loc = nn.Linear(z_dim, z_dim)\n",
    "\n",
    "        # modify the default initialization of lin_z_to_loc\n",
    "        # so that it's starts out as the IDENTITY FUNCTION\n",
    "        self.lin_z_to_loc.weight.data = torch.eye(z_dim)\n",
    "        self.lin_z_to_loc.bias.data = torch.zeros(z_dim)\n",
    "\n",
    "        # three non linear\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "\n",
    "    def forward(self, z_t_1):\n",
    "        \"\"\"\n",
    "        Given the latent z_{t-1} corresponding to the time step t-1 we return the mean and scale vectors that parameterize the\n",
    "        (diagonal) gaussian distribution p(z_t | z_{t-1})\n",
    "        \"\"\"\n",
    "        # compute the gating function\n",
    "        _gate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\n",
    "        gate = self.sigmoid(self.lin_gate_hidden_to_z(_gate))\n",
    "\n",
    "        # compute the 'proposed mean'\n",
    "        _proposed_mean = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\n",
    "        proposed_mean = self.lin_proposed_mean_hidden_to_z(_proposed_mean)\n",
    "\n",
    "        # assemble the actual mean used to sample z_t, which mixes a linear transformation of z_{t-1} with the proposed mean\n",
    "        # modulated by the gating function\n",
    "        loc = (1 - gate) * self.lin_z_to_loc(z_t_1) + gate * proposed_mean\n",
    "\n",
    "        # compute the scale used to sample z_t, using the proposed\n",
    "        # mean from above as input. the softplus ensures that scale is positive\n",
    "        scale = self.softplus(self.lin_sig(self.relu(proposed_mean)))\n",
    "\n",
    "        # return loc, scale which can be fed into Normal (mean and covariance of our Gaussian)\n",
    "        return loc, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model (not working)\n",
    "def model(...):\n",
    "    # initializer\n",
    "    z_prev = self.z_0\n",
    "\n",
    "    # sample the latents z and observed x's one time step at a time\n",
    "    for t in range(1, T_max + 1):\n",
    "        # the next two lines of code sample z_t ~ p(z_t | z_{t-1}).\n",
    "        # first compute the parameters of the diagonal gaussian\n",
    "        # distribution p(z_t | z_{t-1})\n",
    "        z_loc, z_scale = self.trans(z_prev)\n",
    "        # then sample z_t according to dist.Normal(z_loc, z_scale)\n",
    "        z_t = pyro.sample(\"z_%d\" % t, dist.Normal(z_loc, z_scale))\n",
    "\n",
    "        # compute the probabilities that parameterize the bernoulli likelihood\n",
    "        emission_probs_t = self.emitter(z_t)\n",
    "        # the next statement instructs pyro to observe x_t according to the\n",
    "        # bernoulli distribution p(x_t|z_t)\n",
    "        pyro.sample(\"obs_x_%d\" % t,\n",
    "                    dist.Bernoulli(emission_probs_t),\n",
    "                    obs=mini_batch[:, t - 1, :])\n",
    "        # the latent sampled at this time step will be conditioned upon\n",
    "        # in the next time step so keep track of it\n",
    "        z_prev = z_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent example\n",
    "\n",
    "To define a hidden Markov Model (HMM) for this scenario, we need to identify the key components of an HMM:\n",
    "1.\tStates: In our case, the state is whether the agent is in a shocked state or not. We will use 0 to represent a non-shocked state and 1 to represent a shocked state.\n",
    "2.\tObservations: The observation is whether the agent sees an aggressive or non-aggressive image. We will use 0 to represent a non-aggressive image and 1 to represent an aggressive image.\n",
    "3.\tTransition probabilities: The transition probabilities determine the probability of moving from one state to another. In our case, the transition probabilities will depend on the current state and the shock received. Specifically, if the agent is in a non-shocked state and receives a shock, there is a high probability of transitioning to the shocked state. Conversely, if the agent is in the shocked state and does not receive a shock, there is a high probability of transitioning to the non-shocked state.\n",
    "4.\tEmission probabilities: The emission probabilities determine the probability of observing a particular image given the current state. In our case, the emission probabilities will depend on the current state. Specifically, if the agent is in the non-shocked state, there is a high probability of observing a non-aggressive image, and if the agent is in the shocked state, there is a high probability of observing an aggressive image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import Trace_ELBO, SVI\n",
    "\n",
    "\n",
    "def transition_model(state):\n",
    "    p_stay = 0.7 if state else 0.9\n",
    "    p_switch = 1 - p_stay\n",
    "    return pyro.sample('state_transition', dist.Categorical(torch.tensor([p_stay, p_switch])))\n",
    "\n",
    "\n",
    "def emission_model(state):\n",
    "    p_emission = 0.9 if state else 0.1\n",
    "    return pyro.sample('emission', dist.Bernoulli(torch.tensor(p_emission)))\n",
    "\n",
    "\n",
    "def initial_model():\n",
    "    return pyro.sample('initial_state', dist.Categorical(torch.tensor([0.5, 0.5])))\n",
    "\n",
    "\n",
    "def model(stimuli, shock, initial_state):\n",
    "    states = [pyro.sample('init_state', dist.Categorical(initial_state))]\n",
    "    emissions = [emission_model(states[0]).item()]\n",
    "    for i in range(1, len(stimuli)):\n",
    "        states.append(transition_model(states[i-1]).item())\n",
    "        emissions.append(emission_model(states[i]).item())\n",
    "    return states, emissions\n",
    "\n",
    "\n",
    "def guide(stimuli, shock, initial_state, states, emissions):\n",
    "    for i in range(len(stimuli)):\n",
    "        states[i] = pyro.sample('state_{}'.format(i), dist.Bernoulli(torch.tensor(0.5)))\n",
    "        emissions[i] = pyro.sample('emission_{}'.format(i), dist.Bernoulli(torch.tensor(0.5)))\n",
    "\n",
    "\n",
    "stimuli = torch.tensor([0, 0, 0, 1, 1, 0, 1, 1, 0])\n",
    "shock = torch.tensor([0, 0, 0, 1, 1, 0, 0, 1, 0])\n",
    "initial_state = torch.tensor([0.5, 0.5])\n",
    "\n",
    "optimizer = pyro.optim.Adam({'lr': 0.1})\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "for i in range(1000):\n",
    "    states, emissions = model(stimuli, shock, initial_state)\n",
    "    loss = svi.step(stimuli, shock, initial_state, states, emissions)\n",
    "    if i % 100 == 0:\n",
    "        print(\"step {}: loss = {}\".format(i, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rational agent discretisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valid_subjects = extract_correct_csv.extract_only_valid_subject()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_array_simulated = np.load('output/pyro/complete_rational/total.npy',allow_pickle=True)\n",
    "total_array_simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "values = np.array([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "discretized_data = np.digitize(total_array_simulated, values)\n",
    "print(discretized_data.shape)\n",
    "\n",
    "\n",
    "df_global = pd.DataFrame(columns=['Subject', 'Rating rational', 'Rating real'])\n",
    "\n",
    "for sub in valid_sub:\n",
    "    string_sub = extract_correct_csv.read_correct_subject_csv(sub)\n",
    "    df_sub = pd.read_csv('data/newLookAtMe/newLookAtMe'+string_sub+'.csv')\n",
    "    df_sub = df_sub[16:]\n",
    "    tmp_df = pd.DataFrame({'Subject': sub, 'Rating rational': discretized_data, 'Rating real': df_sub['rating']})\n",
    "    df_global = pd.concat([df_global, tmp_df])\n",
    "\n",
    "df_global = df_global.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# useless up to now\n",
    "'''df_global = df_global.groupby('Subject', as_index=False).agg({'Rating rational': lambda x: x.tolist(), 'Rating real': lambda x: x.tolist()})\n",
    "df_global['Rating rational'] = df_global['Rating rational'].apply(lambda x: np.array(x))\n",
    "df_global['Rating real'] = df_global['Rating real'].apply(lambda x: np.array(x))'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df_global"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyirr import read_data, intraclass_correlation\n",
    "data = read_data(\"anxiety\")  # loads example data\n",
    "intraclass_correlation(data, \"twoway\", \"agreement\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sub_ = df_global[df_global.Subject == 5].dropna().drop(columns=['Subject']).reset_index(drop=True)\n",
    "df_sub_['Rating rational'] = df_sub_['Rating rational'].astype(float)\n",
    "intraclass_correlation(df_sub_)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
